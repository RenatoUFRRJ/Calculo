{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "digits = load_digits()\n",
    "\n",
    "\n",
    "#print(digits.data.shape)\n",
    "clf = MLPClassifier(activation='relu', alpha=0.00001, batch_size='auto', beta_1=0.9,\n",
    "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
    "       learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
    "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
    "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
    "       verbose=True, warm_start=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = digits.data, digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 9.86269564\n",
      "Iteration 2, loss = 4.36978860\n",
      "Iteration 3, loss = 2.67287522\n",
      "Iteration 4, loss = 1.36381622\n",
      "Iteration 5, loss = 0.81700300\n",
      "Iteration 6, loss = 0.52626515\n",
      "Iteration 7, loss = 0.38585069\n",
      "Iteration 8, loss = 0.31316546\n",
      "Iteration 9, loss = 0.26189788\n",
      "Iteration 10, loss = 0.23227991\n",
      "Iteration 11, loss = 0.20586189\n",
      "Iteration 12, loss = 0.18579333\n",
      "Iteration 13, loss = 0.16915340\n",
      "Iteration 14, loss = 0.15601222\n",
      "Iteration 15, loss = 0.14318893\n",
      "Iteration 16, loss = 0.13267387\n",
      "Iteration 17, loss = 0.12309866\n",
      "Iteration 18, loss = 0.11514087\n",
      "Iteration 19, loss = 0.10787718\n",
      "Iteration 20, loss = 0.10171450\n",
      "Iteration 21, loss = 0.09610335\n",
      "Iteration 22, loss = 0.08925789\n",
      "Iteration 23, loss = 0.08528493\n",
      "Iteration 24, loss = 0.08080782\n",
      "Iteration 25, loss = 0.07633944\n",
      "Iteration 26, loss = 0.07460164\n",
      "Iteration 27, loss = 0.06851509\n",
      "Iteration 28, loss = 0.06604125\n",
      "Iteration 29, loss = 0.06284214\n",
      "Iteration 30, loss = 0.06274679\n",
      "Iteration 31, loss = 0.05630733\n",
      "Iteration 32, loss = 0.05417012\n",
      "Iteration 33, loss = 0.05194362\n",
      "Iteration 34, loss = 0.04967775\n",
      "Iteration 35, loss = 0.04776762\n",
      "Iteration 36, loss = 0.04557645\n",
      "Iteration 37, loss = 0.04367520\n",
      "Iteration 38, loss = 0.04191431\n",
      "Iteration 39, loss = 0.04019334\n",
      "Iteration 40, loss = 0.03874898\n",
      "Iteration 41, loss = 0.03719065\n",
      "Iteration 42, loss = 0.03585315\n",
      "Iteration 43, loss = 0.03466596\n",
      "Iteration 44, loss = 0.03296970\n",
      "Iteration 45, loss = 0.03197692\n",
      "Iteration 46, loss = 0.03097042\n",
      "Iteration 47, loss = 0.03089039\n",
      "Iteration 48, loss = 0.02904007\n",
      "Iteration 49, loss = 0.02768600\n",
      "Iteration 50, loss = 0.02694460\n",
      "Iteration 51, loss = 0.02604557\n",
      "Iteration 52, loss = 0.02508161\n",
      "Iteration 53, loss = 0.02447849\n",
      "Iteration 54, loss = 0.02375374\n",
      "Iteration 55, loss = 0.02289097\n",
      "Iteration 56, loss = 0.02220260\n",
      "Iteration 57, loss = 0.02163105\n",
      "Iteration 58, loss = 0.02060215\n",
      "Iteration 59, loss = 0.02010356\n",
      "Iteration 60, loss = 0.01948269\n",
      "Iteration 61, loss = 0.01892661\n",
      "Iteration 62, loss = 0.01833612\n",
      "Iteration 63, loss = 0.01803010\n",
      "Iteration 64, loss = 0.01743890\n",
      "Iteration 65, loss = 0.01692673\n",
      "Iteration 66, loss = 0.01643896\n",
      "Iteration 67, loss = 0.01605643\n",
      "Iteration 68, loss = 0.01560541\n",
      "Iteration 69, loss = 0.01513987\n",
      "Iteration 70, loss = 0.01469732\n",
      "Iteration 71, loss = 0.01451226\n",
      "Iteration 72, loss = 0.01402046\n",
      "Iteration 73, loss = 0.01371693\n",
      "Iteration 74, loss = 0.01334230\n",
      "Iteration 75, loss = 0.01305509\n",
      "Iteration 76, loss = 0.01288101\n",
      "Iteration 77, loss = 0.01246173\n",
      "Iteration 78, loss = 0.01208172\n",
      "Iteration 79, loss = 0.01183723\n",
      "Iteration 80, loss = 0.01160188\n",
      "Iteration 81, loss = 0.01115598\n",
      "Iteration 82, loss = 0.01101865\n",
      "Iteration 83, loss = 0.01074927\n",
      "Iteration 84, loss = 0.01047218\n",
      "Iteration 85, loss = 0.01002888\n",
      "Iteration 86, loss = 0.00986260\n",
      "Iteration 87, loss = 0.00971563\n",
      "Iteration 88, loss = 0.00949105\n",
      "Iteration 89, loss = 0.00918723\n",
      "Iteration 90, loss = 0.00891616\n",
      "Iteration 91, loss = 0.00876049\n",
      "Iteration 92, loss = 0.00851351\n",
      "Iteration 93, loss = 0.00830157\n",
      "Iteration 94, loss = 0.00813138\n",
      "Iteration 95, loss = 0.00785391\n",
      "Iteration 96, loss = 0.00769266\n",
      "Iteration 97, loss = 0.00753907\n",
      "Iteration 98, loss = 0.00743741\n",
      "Iteration 99, loss = 0.00729999\n",
      "Iteration 100, loss = 0.00706625\n",
      "Iteration 101, loss = 0.00679706\n",
      "Iteration 102, loss = 0.00674532\n",
      "Iteration 103, loss = 0.00654724\n",
      "Iteration 104, loss = 0.00641233\n",
      "Iteration 105, loss = 0.00634052\n",
      "Iteration 106, loss = 0.00620547\n",
      "Iteration 107, loss = 0.00614600\n",
      "Iteration 108, loss = 0.00593753\n",
      "Iteration 109, loss = 0.00591352\n",
      "Iteration 110, loss = 0.00569921\n",
      "Iteration 111, loss = 0.00555210\n",
      "Iteration 112, loss = 0.00547753\n",
      "Iteration 113, loss = 0.00537239\n",
      "Iteration 114, loss = 0.00525155\n",
      "Iteration 115, loss = 0.00513855\n",
      "Iteration 116, loss = 0.00505510\n",
      "Iteration 117, loss = 0.00493885\n",
      "Iteration 118, loss = 0.00485568\n",
      "Iteration 119, loss = 0.00481226\n",
      "Iteration 120, loss = 0.00468632\n",
      "Iteration 121, loss = 0.00461217\n",
      "Iteration 122, loss = 0.00455425\n",
      "Iteration 123, loss = 0.00444989\n",
      "Iteration 124, loss = 0.00440570\n",
      "Iteration 125, loss = 0.00430307\n",
      "Iteration 126, loss = 0.00422798\n",
      "Iteration 127, loss = 0.00417359\n",
      "Iteration 128, loss = 0.00409212\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=True, warm_start=True)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true, y_pred, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97333333333333338"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X, y = load_iris(return_X_y=True)\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs',\n",
    "                         multi_class='multinomial').fit(X, y)\n",
    "clf.predict(X[:2, :])\n",
    "\n",
    "clf.predict_proba(X[:2, :]) \n",
    "\n",
    "\n",
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
